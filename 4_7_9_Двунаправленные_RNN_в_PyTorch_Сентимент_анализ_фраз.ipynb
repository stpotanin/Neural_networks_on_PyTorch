{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZGZHLUJplesEQcbSZIHhI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1576205/step/9"
      ],
      "metadata": {
        "id": "s20R_2RUFTk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Глобальные переменные\n",
        "_global_words_0 = ['аа', 'аатаа', 'аба', 'абба', 'абиба', 'ава', 'аваава', 'авава', 'авва', 'ага', 'агга', 'ада', 'адда', 'ажа', 'ака', 'акака', 'аканака', 'акика', 'акка', 'ала', 'алала', 'алафала', 'амакама', 'амма', 'ана', 'анапана', 'анасана', 'анатана', 'анисина', 'анна', 'анона', 'апа', 'апипа', 'аппа', 'ара', 'арара', 'арора', 'арра', 'арура', 'аса', 'ата', 'атета', 'атта', 'аулуа', 'афа', 'аха', 'ахаха', 'ахоха', 'ахха', 'аца', 'ацыца', 'ача', 'аша', 'баб', 'бараб', 'батаб', 'бахаб', 'биб', 'боб', 'вёв', 'вив', 'вызыв', 'гачаг', 'гег', 'гиг', 'гириг', 'гог', 'гыг', 'гэг', 'дед', 'дид', 'диороид', 'довод', 'дойод', 'дород', 'доход', 'дуд', 'еае', 'ейе', 'ёре', 'ере', 'ёте', 'жож', 'заказ', 'замаз', 'заз', 'зуз', 'чи', 'и', 'иааи', 'иаи', 'иби', 'иви', 'идииди', 'ижжи', 'изи', 'ики', 'или', 'илли', 'иньни', 'ири', 'ирори', 'ихи', 'ичи', 'ичиичи', 'ишши', 'йай', 'йой', 'каак', 'кабак', 'кавак', 'казак', 'кайак', 'как', 'канак', 'капак', 'карак', 'касак', 'кассак', 'кек', 'кёк', 'келек', 'келлек', 'керек', 'кесек', 'кечёк', 'кибик', 'кижик', 'кизик', 'киик', 'кийик', 'кик', 'килик', 'киллилиллик', 'киник', 'киноник', 'кичик', 'кишик', 'ковок', 'кок', 'коллок', 'колок', 'комок', 'конок', 'копок', 'коппок', 'корок', 'косок', 'кошок', 'кудук', 'кук', 'кумук', 'курук', 'куук', 'кыйык', 'кык', 'кытык', 'кэк', 'кюк', 'лаал', 'лал', 'лобол', 'лол', 'лыл', 'мадам', 'мазам', 'макам', 'мам', 'манам', 'марам', 'мелем', 'мем', 'мивим', 'мидим', 'миллим', 'мим', 'миним', 'мом', 'моном', 'мум', 'мурум', 'мэм', 'наан', 'набан', 'наган', 'назан', 'накан', 'нан', 'напан', 'насан', 'нашан', 'некен', 'нен', 'ненен', 'нигин', 'нимин', 'нойон', 'нон', 'ноон', 'норурон', 'нэн', 'нян', 'о', 'обибо', 'обо', 'ово', 'оддо', 'ойо', 'око', 'оло', 'ололо', 'оно', 'оо', 'оро', 'ороборо', 'оруро', 'оссо', 'офо', 'очо', 'ошо', 'переп', 'покоп', 'поп', 'посоп', 'потоп', 'пуп', 'радар', 'расар', 'ревер', 'реер', 'рейер', 'ремер', 'репер', 'реппер', 'рер', 'рогор', 'ророр', 'ротатор', 'ротор', 'рэпєр', 'рэппєр', 'сас', 'секес', 'сиис', 'солос', 'соссос', 'статс', 'суккус', 'сукус', 'сус', 'таат', 'такат', 'таннат', 'тартрат', 'тассат', 'тат', 'тауат', 'тидит', 'тиллит', 'тимит', 'тирит', 'тит', 'тихит', 'тозот', 'топот', 'торот', 'тумут', 'тут', 'тыыт', 'у', 'убу', 'уду', 'улу', 'уруушу', 'фараф', 'феф', 'ханнах', 'хенех', 'хох', 'целец', 'чаач', 'чабач', 'чавач', 'чагач', 'чепеч', 'чеч', 'чижич', 'шабаш', 'шалаш', 'шамаш', 'шараш', 'шереш', 'шириш', 'шиш', 'шош', 'шугуш', 'шумуш', 'щэщ', 'эвэ', 'эдэ']\n",
        "_global_words_1 = ['сарпиночник', 'контрабандист', 'мопед', 'вульгарность', 'ятрышник', 'следопыт', 'оперирование', 'шпажист', 'англосаксонец', 'натуралистичность', 'серница', 'раздел', 'памятник', 'антрополог', 'новорождённая', 'окрол', 'гальваноскоп', 'кофта', 'председатель', 'ржанище', 'помилованная', 'примирение', 'суберин', 'папуаска', 'злободневность', 'эпископ', 'неучтивость', 'адат', 'подавание', 'походка', 'хорь', 'брейд-вымпел', 'предпочтение', 'слепушонок', 'кудель', 'эдикт', 'разнеженность', 'духанщик', 'вертолётчица', 'светотехника', 'провозгласитель', 'бериллий', 'пискунья', 'отгонщик', 'глиптодонт', 'локомобиль', 'пресмыкание', 'старобытность', 'двупланность', 'лютеций', 'прирез', 'рявкание', 'перегрузка', 'токсиколог', 'искусительница', 'дикция', 'древность', 'сертификация', 'магистраль', 'фагоцитоз', 'всесторонность', 'армада', 'люэс', 'бутоньерка', 'полустишие', 'сельхозинвентарь', 'огранка', 'минускул', 'монотипист', 'дань', 'бармен', 'выпирание', 'противосияние', 'альтист', 'бекас', 'глиптотека', 'полиграфия', 'уменьшение', 'лункование', 'клирос', 'пагода', 'элементарность', 'предпочтительность', 'горицвет', 'ксилофон', 'игиль', 'паратость', 'ножовщик', 'гель', 'непроизносимость', 'отшвыривание', 'новолуние', 'обрезок', 'технеций', 'самбист', 'инсулин', 'бирманка', 'гвардия', 'папуас', 'оживание', 'заскабливание', 'переливт', 'кройка', 'контроверза', 'ниспровержение', 'нагреватель', 'плата', 'паралитик', 'платан', 'эндокард', 'скликание', 'инвенция', 'раскутывание', 'загустение', 'чека', 'перенагревание', 'припыл', 'тенётчик', 'натюрморист', 'цивилизованность', 'упрощение', 'отопленец', 'свечка', 'предплужник', 'юродивая', 'неприличность', 'вех', 'лежание', 'драчливость', 'фидеист', 'дезодорант', 'прокапчивание', 'сбережение', 'посыльная', 'фольклористика', 'вдохновитель', 'культурница', 'виноградарь', 'пряничник', 'практикант', 'тузлук', 'плач', 'вареник', 'рислинг', 'транш', 'укупорщик', 'усложнение', 'фальшивомонетничество', 'пышность', 'подстановка', 'санитар', 'линовальщик', 'септик', 'пережидание', 'фалл', 'наивность', 'метафизичность', 'вычищение', 'лярд', 'передрессировывание', 'долгожитель', 'метрополитен', 'прошивка', 'подчитывание', 'ёлка', 'подкрутка', 'аил', 'концепция', 'обмол', 'обиженная', 'жертвенник', 'отчизна', 'шёпот', 'обмыливание', 'водохранилище', 'пантовар', 'притачка', 'кардиография', 'навинчивание', 'угнетённость', 'высокопарность', 'ломаная', 'непоследовательность', 'дилетант', 'разгром', 'горло', 'коалиция', 'федералист', 'отдыхающий', 'неудовлетворительность', 'театральность', 'шурфование', 'подгрузка', 'привкус', 'крольчатина', 'ярка', 'декабристка', 'неоклассик', 'откус', 'педфак', 'одежда', 'евпатория', 'индонезия', 'кастрюля', 'качели', 'мамонт', 'копье', 'колледж', 'авиаметеостанция', 'гороскоп', 'марево', 'десница', 'мозоль', 'копоть', 'креветка', 'качалка', 'конвейер', 'алоэ', 'камбуз', 'катализатор', 'ладонь', 'крыло', 'кий', 'амфибия', 'бородавка', 'кафтан', 'стул', 'иордания', 'электричка', 'пещера', 'мундир', 'водоросль', 'бар', 'балерина', 'граната', 'брус', 'купальня', 'башмачок', 'берлин', 'жеребец', 'воробей', 'сова', 'леденец', 'арена', 'узел', 'софа', 'утюг', 'ландыш', 'вакцина', 'бурьян', 'погреб', 'душ', 'гамбург', 'джунгли', 'голень', 'желток', 'лохмотья', 'берег', 'голгофа', 'шкатулка', 'венок', 'малыш', 'кемпинг', 'паркет', 'баня', 'департамент', 'боекомплект', 'канзас', 'дренаж', 'капсула', 'автомагистраль', 'антиквар', 'мотор', 'карамелька', 'лев', 'впадина', 'декада', 'масленка', 'медпункт', 'мультфильм', 'лотерея', 'калория', 'говядина', 'камфара', 'зубок', 'лимузин', 'бильярд', 'колдобина', 'иероглиф', 'воск', 'шпаргалка', 'траншея', 'авиастроитель', 'пряник', 'бром', 'автопоезд', 'кортик', 'дыхание', 'империя', 'плов']\n",
        "len(_global_words_0), len(_global_words_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWdjlA0bxh4f",
        "outputId": "4e654e0a-d6bf-48ed-8f37-622499d68eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "class WordsDataset(data.Dataset):\n",
        "    def __init__(self, batch_size=8):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Формируем объединённый список слов (data) с метками (target)\n",
        "        self.words_lst = [(w, 0) for w in _global_words_0] + [(w, 1) for w in _global_words_1]\n",
        "        self.words_lst.sort(key=lambda x: len(x[0]))\n",
        "        self.dataset_len = len(self.words_lst)\n",
        "\n",
        "        # Подготовка алфавита и one-hot\n",
        "        _text = \"\".join(_global_words_0 + _global_words_1).lower()\n",
        "        self.alphabet = sorted(set(_text))\n",
        "        self.alpha_to_int = {c: i for i, c in enumerate(self.alphabet)}\n",
        "        self.num_characters = len(self.alphabet)\n",
        "        # Создаём eye неквадратную, чтобы -1 → 0:\n",
        "        self.onehots = torch.eye(self.num_characters + 1, self.num_characters)\n",
        "\n",
        "    # формирование и возвращение батча данных по индексу item\n",
        "    def __getitem__(self, item):\n",
        "        item *= self.batch_size\n",
        "        item_last = min(item + self.batch_size, self.dataset_len)\n",
        "        max_len = len(self.words_lst[item_last - 1][0])\n",
        "\n",
        "        # data, target батча\n",
        "        d = [[self.alpha_to_int[c] for c in word[0]] + [-1] * (max_len - len(word[0]))\n",
        "             for word in self.words_lst[item:item_last]]\n",
        "        t = torch.FloatTensor([word[1] for word in self.words_lst[item:item_last]])\n",
        "\n",
        "        data_tensor = torch.zeros(len(d), max_len, self.num_characters)\n",
        "        for i, indices in enumerate(d):\n",
        "            data_tensor[i, :, :] = self.onehots[indices]\n",
        "\n",
        "        return data_tensor, t\n",
        "\n",
        "    # Размер обучающей выборки в батчах\n",
        "    def __len__(self):\n",
        "        return (self.dataset_len + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "\n",
        "class BiRNN_Model(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size, 16, batch_first=True, bidirectional=True)\n",
        "        self.out = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, h = self.rnn(x)\n",
        "        h_bi = torch.cat([h[0], h[1]], dim=1)\n",
        "        return self.out(h_bi)\n",
        "\n",
        "\n",
        "# Обучение\n",
        "d_train = WordsDataset(batch_size=8)\n",
        "train_data = data.DataLoader(d_train, batch_size=1, shuffle=True)\n",
        "\n",
        "model = BiRNN_Model(d_train.num_characters)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.train()\n",
        "for _ in range(2):\n",
        "    for x_train, y_train in train_data:\n",
        "        x_train = x_train.squeeze(0)\n",
        "        y_train = y_train.view(-1, 1)\n",
        "        pred = model(x_train)\n",
        "        loss = loss_func(pred, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Оценка\n",
        "model.eval()\n",
        "Q = 0\n",
        "Q_alter = 0 # для альтернативного подсчёта\n",
        "for x_train, y_train in train_data:\n",
        "    with torch.no_grad():\n",
        "        # Альтернативный метод, не принимается системой, хоть и выдаёт такие же значения:\n",
        "        p = model(x_train.squeeze(0)).sigmoid() >= 0.5 # вычислите прогноз модели для x_train\n",
        "        correct = (p == (y_train > 0.5)) # преобразуем torch.FloatTensor в torch.BoolTensor\n",
        "        Q_alter += correct.float().mean() # вычислите долю верных классификаций для p и y_train\n",
        "\n",
        "        # Аналогичный блок из эталонного решения:\n",
        "        p = model(x_train.squeeze(0))\n",
        "        Q += torch.mean((torch.sign(p.flatten()) == 2 * y_train.flatten() - 1).float())\n",
        "Q /= len(d_train)\n",
        "Q_alter /= len(d_train)\n",
        "Q, Q_alter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq2HoOowhZSm",
        "outputId": "c8356b7c-b68f-4da2-f737-9bd78e92c20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8983), tensor(0.8833))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOn5YD9bFQiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5500c4-ef38-4db5-b0d2-d6a35039a588"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7733)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "class WordsDataset(data.Dataset):\n",
        "    def __init__(self, batch_size=8):\n",
        "        self.batch_size = batch_size\n",
        "        # Формирование порядковых номеров символов\n",
        "        _text = \"\".join(_global_words_0 + _global_words_1).lower()\n",
        "        self.alphabet = set(_text) # набор символов\n",
        "        self.int_to_alpha = dict(enumerate(sorted(self.alphabet))) # число в символ\n",
        "        self.alpha_to_int = {b: a for a, b in self.int_to_alpha.items()} # символ в число\n",
        "        self.alphabet_size = len(self.alphabet)\n",
        "\n",
        "        # Формируем data и target для объединённого списка\n",
        "        words = [[w, 0] for w in _global_words_0] + [[w, 1] for w in _global_words_1]\n",
        "        self.data = sorted(words, key=lambda item: (len(item[0])))\n",
        "        self.length = (len(self.data) + batch_size - 1) // batch_size\n",
        "\n",
        "    # формирование и возвращение батча данных по индексу item\n",
        "    def __getitem__(self, item):\n",
        "        batch = self.data[self.batch_size * item:self.batch_size * (item + 1)]\n",
        "        max_len = max(len(x[0]) for x in batch)\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            # Кодирование слов цифрами и паддинг минус единицей\n",
        "            batch[i][0] = [self.alpha_to_int.get(c, 0) for c in batch[i][0]]\n",
        "            batch[i][0] += [-1] * (max_len - len(batch[i][0]))\n",
        "\n",
        "        # Номера букв - в отдельныый тензор\n",
        "        batch_indices = torch.tensor([x[0] for x in batch])  # (batch_size, seq_len)\n",
        "\n",
        "        # One-Hot:\n",
        "        # Создаём eye неквадратную, чтобы -1 → 0:\n",
        "        eye = torch.eye(self.alphabet_size + 1, self.alphabet_size)  # Последняя строка — паддинг (всё нули)\n",
        "        data = eye[batch_indices]\n",
        "\n",
        "        target = torch.tensor([x[1] for x in batch]).float() # (batch_size, 1)\n",
        "\n",
        "        return data, target\n",
        "\n",
        "\n",
        "    def __len__(self): # возврат размер обучающей выборки в батчах\n",
        "        return self.length\n",
        "\n",
        "# здесь объявляйте класс модели\n",
        "class BiRNN_Model(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size, 16, batch_first=True, bidirectional=True)\n",
        "        self.out = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, h = self.rnn(x)\n",
        "        h_bi = torch.cat([h[0], h[1]], dim=1)\n",
        "        return self.out(h_bi)\n",
        "\n",
        "\n",
        "# Обучение\n",
        "d_train = WordsDataset(batch_size=8)\n",
        "train_data = data.DataLoader(d_train, batch_size=1, shuffle=True)\n",
        "\n",
        "model = BiRNN_Model(d_train.alphabet_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.train()\n",
        "for _ in range(2):\n",
        "    for x_train, y_train in train_data:\n",
        "        x_train = x_train.squeeze(0)\n",
        "        y_train = y_train.view(-1, 1)\n",
        "        pred = model(x_train)\n",
        "        loss = loss_func(pred, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Оценка\n",
        "model.eval()\n",
        "Q = 0\n",
        "for x_train, y_train in train_data:\n",
        "    with torch.no_grad():\n",
        "        # Мой метод, но он не принимается системой (хоть и выдаёт такие же значения):\n",
        "        # p = model(x_train.squeeze(0)).sigmoid() > 0.5 # вычислите прогноз модели для x_train\n",
        "        # correct = (p == (y_train > 0.5)) # преобразуем torch.FloatTensor в torch.BoolTensor\n",
        "        # Q += correct.float().mean() # вычислите долю верных классификаций для p и y_train\n",
        "\n",
        "        # Аналогичный блок из эталонного решения:\n",
        "        p = model(x_train.squeeze(0))\n",
        "        Q += torch.mean((torch.sign(p.flatten()) == 2 * y_train.flatten() - 1).float())\n",
        "Q /= len(d_train)\n",
        "Q"
      ]
    }
  ]
}