{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlL7UJwlVg4HxjuAK8THrb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1576202/step/7"
      ],
      "metadata": {
        "id": "U9ftYPTvPAXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥—É–ª—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –≤ —Ñ–∞–π–ª –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "!pip install navec\n",
        "import requests\n",
        "from navec import Navec\n",
        "\n",
        "url_navec_news = 'https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "\n",
        "response = requests.get(url_navec_news, stream=True)\n",
        "with open('navec_hudlit_v1_12B_500K_300d_100q.tar', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "global_navec = Navec.load(path)"
      ],
      "metadata": {
        "id": "INUyuPzbTDeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcb06f9-f589-440d-ca3d-a0bc0c9bb571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: navec in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from navec) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã —Å–ª–æ–≤–∞—Ä—è\n",
        "is_word = '–º–∞—à–∏–Ω–∞' in global_navec # –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
        "v = global_navec['–∫–∞–∂–¥—ã–π'] # embedding —Å–ª–æ–≤–∞ \"–∫–∞–∂–¥—ã–π\"\n",
        "indx = global_navec.vocab['–∫–∞–∂–¥—ã–π'] # –∏–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞ \"–∫–∞–∂–¥—ã–π\" –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
        "w = global_navec.vocab.words[indx] # —Å–ª–æ–≤–æ –ø–æ –∏–Ω–¥–µ–∫—Å—É\n",
        "is_word, v.shape, indx, w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4jq-X4WjAIH",
        "outputId": "c56257d1-d0e2-4711-8ff6-207fedb77d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, (300,), 161025, '–∫–∞–∂–¥—ã–π')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–≤–≤–∞–Ω–∏—è\n",
        "_global_var_text = [\n",
        "\"–ö–∞–∫ —è –æ—Ç–º–µ—á–∞–ª –≤–æ –≤–≤–µ–¥–µ–Ω–∏–∏ –ø—Ä–æ—Å—Ç–µ–π—à–∞—è –ù–° –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π\",\n",
        "\"–≠—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä –Ω–µ–π—Ä–æ —Å–µ—Ç–∏\",\n",
        "\"–ö–∞–∂–¥–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –∏–º–µ–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π\"\n",
        "]"
      ],
      "metadata": {
        "id": "Iz5jKJIzVvKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I69r49onO1sA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ecc777-b466-4446-e39a-4b0a6725f5f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "—Ç–∞–∫–∏–º–∏ –±—ã–ª–∏ –ø–µ—Ä–≤—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ —Ä–∞–∑–Ω—ã–π —Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Ç–æ–≤–∞—è –Ω–µ–¥–æ–≤–µ—Ä–∏—é –∞–º–∏—Ç–∞ –æ—Ç–æ–º—â–µ–Ω–∞ mag –ø–∏—Ç–µ—Äc –æ–±–∏–¥–∏–º –≤—ã—Ä—É—á–∞—é—Ç\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "# —Å—é–¥–∞ –∫–æ–ø–∏—Ä—É–π—Ç–µ –∫–ª–∞—Å—Å WordsDataset, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –∑–∞–Ω—è—Ç–∏–∏\n",
        "class WordsDataset(data.Dataset):\n",
        "    def __init__(self, prev_words=4):\n",
        "        super().__init__()\n",
        "        self.text = _global_var_text\n",
        "        df = []\n",
        "        for string in self.text:\n",
        "            s = [word for word in string.lower().split()]\n",
        "            for i in range(prev_words, len(s)):\n",
        "                df.append(s[i-prev_words:i+1])\n",
        "        self.df = df\n",
        "        self.length = len(self.df)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, indx):\n",
        "            prev_words = self.df[indx][:-1]\n",
        "            prev_words_emb = torch.stack([torch.as_tensor(global_navec[word]) for word in prev_words])\n",
        "            next_word = self.df[indx][-1]\n",
        "            next_word_indx = global_navec.vocab[next_word]\n",
        "\n",
        "            return prev_words_emb, torch.tensor(next_word_indx)\n",
        "\n",
        "# –∑–¥–µ—Å—å –æ–±—ä—è–≤–ª—è–π—Ç–µ –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏\n",
        "class Words_Model(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn = nn.RNN(in_features, 16, batch_first=True)\n",
        "        self.linear = nn.Linear(32, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.emb(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = torch.cat((x[:, -1, :], x[:, 0, :]), dim=-1)\n",
        "        return self.linear(x)\n",
        "\n",
        "# —Å—é–¥–∞ –∫–æ–ø–∏—Ä—É–π—Ç–µ –æ–±—ä–µ–∫—Ç—ã d_train –∏ train_data, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –∑–∞–Ω—è—Ç–∏–∏\n",
        "d_train = WordsDataset(prev_words=4)\n",
        "\n",
        "train_data = data.DataLoader(d_train, batch_size=8, shuffle=True)\n",
        "\n",
        "'''–ó–î–ï–°–¨ –ö–õ–Æ–ß–ï–í–´–ï –†–ê–°–•–û–ñ–î–ï–ù–ò–Ø –° –≠–¢–ê–õ–û–ù–ù–´–ú –†–ï–®–ï–ù–ò–ï–ú'''\n",
        "# model = WordsRNN(100, len(global_navec.vocab)) # —Å—Ç—Ä–æ–∫–∞ –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è\n",
        "model = Words_Model(300, len(global_navec.vocab.words))\n",
        "\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.01)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 1 # —á–∏—Å–ª–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –æ—Ç 100 –∏ –±–æ–ª–µ–µ)\n",
        "model.train()\n",
        "\n",
        "for _e in range(epochs):\n",
        "    # —Å –ø–æ–º–æ—â—å—é —Ü–∏–∫–ª–∞ for –ø–µ—Ä–µ–±–µ—Ä–∏—Ç–µ –±–∞—Ç—á–∏ –∏–∑ –æ–±—ä–µ–∫—Ç–∞ train_data\n",
        "    for x_train, y_train in train_data:\n",
        "        predict = model(x_train).squeeze()\n",
        "        loss = loss_func(predict, y_train)\n",
        "\n",
        "        # –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è (–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# –ø–µ—Ä–µ–≤–µ–¥–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏\n",
        "model.eval()\n",
        "predict = \"–¢–∞–∫–∏–º–∏ –±—ã–ª–∏ –ø–µ—Ä–≤—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ\".lower().split()\n",
        "total = 10 # —á–∏—Å–ª–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã—Ö —Å–ª–æ–≤ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ—Ä–∞–∑–µ)\n",
        "\n",
        "for _ in range(total):\n",
        "    emb = torch.stack([torch.as_tensor(global_navec[word]) for word in predict[-4:]])\n",
        "    indx = model(emb.unsqueeze(0)).argmax().item() # unsqueeze(0) –¥–ª—è batch size\n",
        "    # print(indx) # –≥–ª—è–Ω—É—Ç—å –Ω–∞ –∏–Ω–¥–µ–∫—Å\n",
        "    predict.append(global_navec.vocab.words[indx])\n",
        "\n",
        "# –≤—ã–≤–µ–¥–∏—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –Ω–∞ —ç–∫—Ä–∞–Ω\n",
        "predict = \" \".join(predict)\n",
        "print(predict)\n",
        "\n",
        "# ['—Ç–∞–∫–∏–º–∏', '–±—ã–ª–∏', '–ø–µ—Ä–≤—ã–µ', '–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ', '—Å–µ—Ç–∏', '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ', 'ukraina', '–∑–∞–º–µ—Ç–∏–≤—à–∏–π', '—ç—Ñ–∏—Ä–Ω—ã–µ', 'tape', '–∞–Ω–æ—Ä—Ç–æ—Å–∏—Å', 'zmar', '–Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ', '—Å–æ—Ü–∏–∞–ª—å–Ω–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–º–∏', '—à–∏–ª–æ–≤–æ–π', '–ø–æ–¥–ª–∏–Ω–Ω–æ–π']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/selfedu-rus/neuro-pytorch/blob/main/solves/4.5.5\n",
        "# @title –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–æ—Å—Ç–∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class WordsDataset(data.Dataset):\n",
        "    def __init__(self, navec_emb, prev_words=4):\n",
        "        self.prev_words = prev_words\n",
        "        self.navec_emb = navec_emb\n",
        "\n",
        "        self.lines = _global_var_text\n",
        "        self.vocab = set((\" \".join(self.lines)).lower().split())\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        data = []\n",
        "        targets = []\n",
        "\n",
        "        for t in self.lines:\n",
        "            words = t.lower().split()\n",
        "            for item in range(len(words)-self.prev_words):\n",
        "                data.append([self.navec_emb[words[x]].tolist() for x in range(item, item + self.prev_words)])\n",
        "                targets.append(self.navec_emb.vocab[words[item+self.prev_words]])\n",
        "\n",
        "        self.data = torch.tensor(data)\n",
        "        self.targets = torch.tensor(targets)\n",
        "\n",
        "        self.length = len(data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.data[item], self.targets[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "class WordsRNN(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.hidden_size = 16\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.rnn = nn.RNN(in_features, self.hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(self.hidden_size, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, h = self.rnn(x)\n",
        "        y = self.out(h)\n",
        "        return y\n",
        "\n",
        "\n",
        "d_train = WordsDataset(global_navec)\n",
        "train_data = data.DataLoader(d_train, batch_size=8, shuffle=True)\n",
        "\n",
        "int_to_word = dict(enumerate((global_navec.vocab)))\n",
        "\n",
        "# –ó–¥–µ—Å—å –º–µ–Ω—è–µ–º:\n",
        "# model = WordsRNN(100, len(global_navec.vocab)) # —ç—Ç–∞–ª–æ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ\n",
        "model = WordsRNN(300, len(int_to_word))\n",
        "\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 1\n",
        "model.train()\n",
        "\n",
        "for _e in range(epochs):\n",
        "    for x_train, y_train in train_data:\n",
        "        predict = model(x_train).squeeze(0)\n",
        "        loss = loss_func(predict, y_train.long())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "predict = \"–¢–∞–∫–∏–º–∏ –±—ã–ª–∏ –ø–µ—Ä–≤—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ\".lower().split()\n",
        "total = 10\n",
        "\n",
        "for _ in range(total):\n",
        "    _data = torch.tensor([d_train.navec_emb[predict[-x]].tolist() for x in range(d_train.prev_words, 0, -1)])\n",
        "    with torch.no_grad():\n",
        "        p = model(_data.unsqueeze(0)).squeeze(0)\n",
        "    indx = torch.argmax(p, dim=1)\n",
        "    predict.append(int_to_word[indx.item()])\n",
        "\n",
        "predict = \" \".join(predict)\n",
        "print(predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "mmis3xVwlEV9",
        "outputId": "ec3eadec-13ee-441e-9cdb-a0181baf7ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'Vocab' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-790f9ee5e60c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_navec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'Vocab' has no len()"
          ]
        }
      ]
    }
  ]
}