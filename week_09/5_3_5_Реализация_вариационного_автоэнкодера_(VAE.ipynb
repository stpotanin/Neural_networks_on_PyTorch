{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaeENEGBXUt/9dY0Kv4DJT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1576210/step/5"
      ],
      "metadata": {
        "id": "rJlu1MCt43r8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T_60YAP40rF",
        "outputId": "f2659530-0129-4c6e-ce89-6f49e3e26709"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5327744483947754"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class KLLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, h_mean, h_log_var):\n",
        "        # h_mean, h_log_var: тензоры размером (batch_size, latent_dim)\n",
        "        # Вычисляем KL для каждого элемента батча и усредняем\n",
        "        kl_divergence = -0.5 * torch.sum(1 + h_log_var - h_mean**2 - torch.exp(h_log_var), dim=1).mean()\n",
        "\n",
        "        return kl_divergence\n",
        "\n",
        "batch_size = 5\n",
        "h_mean = torch.rand(batch_size, 10)\n",
        "h_log_var = torch.rand(batch_size, 10)\n",
        "\n",
        "model = KLLoss()\n",
        "model.eval()\n",
        "\n",
        "loss = model(h_mean, h_log_var).item()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def kl_divergence(h_mean, h_log_var):\n",
        "    # h_mean, h_log_var: тензоры размером (batch_size, latent_dim)\n",
        "    # Вычисляем KL для каждого элемента батча\n",
        "    kl = -0.5 * torch.sum(1 + h_log_var - h_mean.pow(2) - torch.exp(h_log_var), dim=1)\n",
        "    return kl  # размер (batch_size,)\n",
        "\n",
        "# Пример использования\n",
        "batch_size = 5\n",
        "latent_dim = 10\n",
        "h_mean = torch.rand(batch_size, latent_dim)\n",
        "h_log_var = torch.rand(batch_size, latent_dim)\n",
        "\n",
        "kl_values = kl_divergence(h_mean, h_log_var)\n",
        "print(kl_values)  # KL для каждого элемента батча\n",
        "print(kl_values.mean())  # Средний KL по батчу\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5UfCPfGEArN",
        "outputId": "8945ad0f-e8ba-4c18-a49f-e9465a4ef539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.7039, 2.8855, 3.1516, 1.9499, 1.6161])\n",
            "tensor(2.4614)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разберём подробно связь между двумя формулами KL-дивергенции и объясним по каждому слагаемому, почему они выглядят по-разному.\n",
        "\n",
        "---\n",
        "\n",
        "## Формулы\n",
        "\n",
        "1) Классическая формула KL-дивергенции между двумя многомерными нормальными распределениями (в частности между $q = \\mathcal{N}(\\mu_G, \\Sigma_G)$ и стандартным нормальным $p = \\mathcal{N}(0, I)$):\n",
        "\n",
        "$$\n",
        "D_{KL} = \\frac{1}{2} \\left( \\operatorname{tr}(\\Sigma_G) + \\mu_G^T \\mu_G - n - \\log \\det \\Sigma_G \\right)\n",
        "$$\n",
        "\n",
        "2) Часто используемая в VAE формула, когда ковариационная матрица диагональна и задаётся через вектор логарифмов дисперсий $h_{log\\_var}$, а вектор средних $h_{mean}$:\n",
        "\n",
        "$$\n",
        "D_{KL}(q(h) \\| p(h)) = -\\frac{1}{2} \\sum_{j=1}^n \\left(1 + h_{log\\_var,j} - h_{mean,j}^2 - \\exp(h_{log\\_var,j}) \\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Связь и разбор по слагаемым\n",
        "\n",
        "\n",
        "| № | Классическая формула (матрицы) | VAE-формула (поэлементно) | Объяснение |\n",
        "|-|-|-|-|\n",
        "|1| $\\operatorname{tr}(\\Sigma_G)$ | $\\sum_{j=1}^n \\exp(h_{log\\_var,j})$ | След ковариационной матрицы равен сумме диагональных элементов. В VAE ковариационная матрица диагональна, и диагональные элементы - это дисперсии $\\sigma_j^2 = \\exp(h_{log\\_var,j})$. Поэтому $\\operatorname{tr}(\\Sigma_G) = \\sum_j \\sigma_j^2 = \\sum_j \\exp(h_{log\\_var,j})$. |\n",
        "|2| $\\mu_G^T \\mu_G = \\sum_{j=1}^n \\mu_{G,j}^2$ | $\\sum_{j=1}^n h_{mean,j}^2$ | Вектор средних в классической формуле - это просто сумма квадратов средних по всем признакам. В VAE это то же самое. |\n",
        "|3| $- n$ | $\\sum_{j=1}^n 1$ (внутри скобок: единица) | В классической формуле вычитается размерность $n$, а в VAE формуле в сумме по признакам внутри скобок стоит единица для каждого признака. Это одно и то же: $\\sum_{j=1}^n 1 = n$. |\n",
        "|4| $- \\log \\det \\Sigma_G$ | $\\sum_{j=1}^n h_{log\\_var,j}$ | Логарифм определителя диагональной матрицы равен сумме логарифмов диагональных элементов: $\\log \\det \\Sigma_G = \\sum_j \\log \\sigma_j^2 = \\sum_j h_{log\\_var,j}$. В VAE формуле стоит с плюсом, но внутри скобок стоит $- h_{log\\_var,j}$, что эквивалентно $- \\log \\det \\Sigma_G$. |\n",
        "\n",
        "---\n",
        "\n",
        "## Почему появилась единица и куда делась $n$?\n",
        "\n",
        "- В классической формуле $-n$ - это вычитание размерности.\n",
        "- В VAE формуле это реализовано как сумма единиц по размерности: $\\sum_{j=1}^n 1 = n$.\n",
        "- Поэтому единица в скобках - это просто поэлементное представление вычитания размерности.\n",
        "\n",
        "---\n",
        "\n",
        "## Откуда взялся квадрат в третьем слагаемом?\n",
        "\n",
        "- Квадрат - это $\\mu_j^2$, квадрат среднего по j-му признаку.\n",
        "- В классической формуле это $\\mu_G^T \\mu_G = \\sum_j \\mu_j^2$.\n",
        "- В VAE формуле это $- h_{mean,j}^2$ внутри скобок, что соответствует тому же слагаемому.\n",
        "\n",
        "---\n",
        "\n",
        "## Откуда взялось $\\exp(h_{log\\_var,j})$?\n",
        "\n",
        "- В VAE ковариационная матрица диагональна, и дисперсии задаются через логарифм дисперсии: $h_{log\\_var,j} = \\log \\sigma_j^2$.\n",
        "- Чтобы получить дисперсию $\\sigma_j^2$, нужно взять экспоненту: $\\sigma_j^2 = \\exp(h_{log\\_var,j})$.\n",
        "- В классической формуле стоит $\\operatorname{tr}(\\Sigma_G) = \\sum_j \\sigma_j^2$, в VAE - $\\sum_j \\exp(h_{log\\_var,j})$.\n",
        "\n",
        "---\n",
        "\n",
        "## Итог: обе формулы эквивалентны\n",
        "\n",
        "- Классическая формула - матричная, с ковариационной матрицей $\\Sigma_G$.\n",
        "- VAE-формула - поэлементная, с диагональной ковариационной матрицей, заданной через вектор логарифмов дисперсий.\n",
        "- Минус перед формулой VAE связан с тем, что часто KL-дивергенция входит в функцию потерь с минусом (см. ELBO), но по сути это тот же KL.\n",
        "\n",
        "---\n",
        "\n",
        "Если кратко:\n",
        "\n",
        "| Классическая формула | VAE-формула | Комментарий |\n",
        "|-|-|-|\n",
        "| $\\operatorname{tr}(\\Sigma_G)$ | $\\sum_j \\exp(h_{log\\_var,j})$ | След ковариации |\n",
        "| $\\mu_G^T \\mu_G$ | $\\sum_j h_{mean,j}^2$ | Квадрат нормы среднего |\n",
        "| $-n$ | $\\sum_j 1$ | Размерность |\n",
        "| $-\\log \\det \\Sigma_G$ | $\\sum_j -h_{log\\_var,j}$ | Логарифм определителя диагональной матрицы |\n",
        "\n",
        "---\n",
        "\n",
        "Citations:\n",
        "\n",
        "[1] https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)\n",
        "\n",
        "[2] https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij\n",
        "\n",
        "[3] https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0\n",
        "\n",
        "[4] https://yu-xuan.livejournal.com/145896.html\n",
        "\n",
        "[5] https://asu.tusur.ru/learning/090401e/d02/090401e-d02-labs.pdf\n",
        "\n",
        "[6] https://habr.com/ru/companies/ods/articles/322514/\n",
        "\n",
        "[7] http://www.machinelearning.ru/wiki/images/6/6f/Yudin20msc.pdf\n",
        "\n",
        "[8] https://inis.iaea.org/records/5kc0x-fag74/files/16081948.pdf?download=1\n",
        "\n",
        "---\n",
        "Answer from Perplexity: pplx.ai/share"
      ],
      "metadata": {
        "id": "nJINCY9u6830"
      }
    }
  ]
}
