{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmhUmwcQF3ezCJK9sJ9JAm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1576199/step/10"
      ],
      "metadata": {
        "id": "VgdPOtxEY66Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjPRnjcdYqTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f40b0e-a846-45ca-a5a1-1605ff88d83c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class OutputToLinear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, hidden_state = x\n",
        "        # hidden_state.shape: (num_layers, batch_size, hidden_size)\n",
        "        # print(hidden_state.shape) # [2, 18, 25]\n",
        "        return hidden_state[-1]  # берем только последний слой\n",
        "\n",
        "# тензор x в программе не менять\n",
        "batch_size = 18\n",
        "seq_length = 21\n",
        "in_features = 5\n",
        "x = torch.rand(batch_size, seq_length, in_features)\n",
        "\n",
        "num_layers = 2    # Количество слоев RNN\n",
        "hidden_size = 25  # Размер вектора скрытого состояния в каждом рекуррентном слое\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.RNN(in_features, hidden_size, num_layers=num_layers, batch_first=True),\n",
        "    OutputToLinear(),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(25, 5)\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "predict = model(x)\n",
        "\n",
        "predict.shape # [18, 5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Вариант 2: Объединить все слои hidden_state\n",
        "class OutputToLinear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, hidden_state = x\n",
        "        hidden_state = hidden_state.view(hidden_state.shape[1], -1)  # (batch_size, num_layers * hidden_size)\n",
        "        return hidden_state\n",
        "\n",
        "num_layers = 2\n",
        "hidden_size = 12  # 2 * 12 = 24 (чтобы было ближе к 25, но не идеально)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.RNN(in_features, hidden_size, num_layers=num_layers, batch_first=True),\n",
        "    OutputToLinear(),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(24, 5)  # Теперь Linear(24, 5) соответствует размерности hidden_state\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "predict = model(x)\n",
        "\n",
        "predict.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogkqH9EGAD9s",
        "outputId": "50473e13-5704-402a-f0e2-69dcc6db795a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}