{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOT5vKjlx3J9+0izdkerp5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/1576210/step/4"
      ],
      "metadata": {
        "id": "5gu8r1irYrr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM9Kfr5aYlue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a121d6-cb43-446c-df0b-18ffb27232e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 16, 16]), torch.Size([1, 7]), torch.Size([1, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as tfs\n",
        "# import torchvision.transforms.v2 as tfs_v2 - недоступен на Stepik\n",
        "\n",
        "# здесь объявляйте класс VAE_CNN\n",
        "class VAE_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            # [batch_size, 3, 16, 16]\n",
        "            nn.Conv2d(3, 16, (3, 3), stride=1, padding=1),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.MaxPool2d((3, 3), stride=2),\n",
        "            nn.Conv2d(16, 4, (3, 3), stride=1, padding=1),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.MaxPool2d((3, 3), stride=2),\n",
        "            nn.Flatten()\n",
        "\n",
        "        )\n",
        "\n",
        "        self.h_mean = nn.Linear(36, 7)      # [batch_size, 7]\n",
        "        self.h_log_var = nn.Linear(36, 7)   # [batch_size, 7]\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # [batch_size, 7]\n",
        "            nn.Linear(7, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (2, 4, 4)),\n",
        "            nn.ConvTranspose2d(2, 8, (2, 2), stride=2, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(8, 1, (2, 2), stride=2, padding=0),\n",
        "            nn.Sigmoid()    # [batch_size, 1, 16, 16]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        h_mean = self.h_mean(self.encoder(x))\n",
        "        h_log_var = self.h_log_var(self.encoder(x))\n",
        "\n",
        "        # Случайные величины с нулевым средним и единичной дисперсией\n",
        "        noise = torch.normal(mean=torch.zeros_like(h_mean), std=torch.ones_like(h_log_var))\n",
        "        h = torch.exp(h_log_var / 2) * noise + h_mean\n",
        "\n",
        "        # decoder\n",
        "        out = self.decoder(h)\n",
        "\n",
        "        return out, h_mean, h_log_var\n",
        "\n",
        "img_pil = Image.new(mode=\"RGB\", size=(64, 78), color=(0, 128, 255))\n",
        "\n",
        "# здесь продолжайте программу\n",
        "img_transform = tfs.Compose([\n",
        "    tfs.CenterCrop(64),\n",
        "    tfs.Resize(16),\n",
        "    tfs.ToTensor()\n",
        "])\n",
        "\n",
        "img = img_transform(img_pil).unsqueeze(0)  # Добавляем батч-размерность\n",
        "\n",
        "model = VAE_CNN()\n",
        "model.eval()\n",
        "\n",
        "out, hm, hlv = model(img)\n",
        "out.shape, hm.shape, hlv.shape"
      ]
    }
  ]
}
